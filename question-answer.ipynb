{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import requests\n",
    "\n",
    "# pdf_path = \"\"\n",
    "\n",
    "# if not os.path.exists(pdf_path):\n",
    "#   print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "#   url = \"\"\n",
    "\n",
    "#   filename = pdf_path\n",
    "\n",
    "#   response = requests.get(url)\n",
    "\n",
    "#   if response.status_code == 200:\n",
    "#       with open(filename, \"wb\") as file:\n",
    "#           file.write(response.content)\n",
    "#       print(f\"The file has been downloaded and saved as {filename}\")\n",
    "#   else:\n",
    "#       print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "# else:\n",
    "#   print(f\"File {pdf_path} exists.\")\n",
    "import fitz\n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").replace(\"/xo\",\" \").strip()\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = []\n",
    "    for page_number, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_token_count\": len(text) / 4,\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "pdf_path=\"books/Atomic-Habits.pdf\"\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp=English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for itm in pages_and_texts:\n",
    "    itm[\"sentence\"]=list(nlp(itm[\"text\"]).sents)\n",
    "    itm[\"sentence\"]=[str(s) for s in itm[\"sentence\"]]\n",
    "    itm[\"sent_count\"]=len(itm[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(pages_and_texts)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sen_sz=10\n",
    "\n",
    "def split_lst(in_lst:list[str],slc_sz:int=num_sen_sz)->list[list[str]]:\n",
    "    return [in_lst[i:i+slc_sz]for i in range(0,len(in_lst),slc_sz)]\n",
    "\n",
    "for itm in pages_and_texts:\n",
    "    itm[\"sent_chunk\"]=split_lst(itm[\"sentence\"])\n",
    "    itm[\"num_chunk\"]=len(itm[\"sent_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pg_chunk=[]\n",
    "for itm in pages_and_texts:\n",
    "    for sent_ch in itm[\"sent_chunk\"]:\n",
    "        chunk_dict={}\n",
    "        chunk_dict[\"page_num\"]=itm[\"page_number\"]\n",
    "        join_chnk=\"\".join(sent_ch).replace(\"  \", \" \").strip()\n",
    "        join_chnk=re.sub(r'\\.([A-Z])',r'. \\1',join_chnk)\n",
    "        chunk_dict[\"sentence_chunk\"]=join_chnk\n",
    "        chunk_dict[\"chunk_char_count\"]=len(join_chnk)\n",
    "        chunk_dict[\"chunk_word_count\"]=len([w for w in join_chnk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"]=len(join_chnk)/4\n",
    "\n",
    "        pg_chunk.append(chunk_dict)\n",
    "len(pg_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(pg_chunk)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token=20\n",
    "pg_chunk = df[df[\"chunk_token_count\"] > min_token].to_dict(orient=\"records\")\n",
    "pg_chunk[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(pg_chunk)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2',device=\"cpu\")\n",
    "embed_model.to(\"cpu\")\n",
    "txt_chnks=[itm[\"sentence_chunk\"]for itm in pg_chunk]\n",
    "txt_chnks_embed=embed_model.encode(txt_chnks,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "emb_dim=txt_chnks_embed.shape[1]\n",
    "index=faiss.IndexFlatL2(emb_dim)\n",
    "index.add(txt_chnks_embed)\n",
    "faiss.write_index(index,\"sentence_emb.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "def pr_wr(text,wr=80):\n",
    "    wrt=textwrap.fill(text,wr)\n",
    "    print(wrt)\n",
    "    \n",
    "def retrieve(query:str,model:SentenceTransformer=embed_model,n:int=4):\n",
    "    query_emb=model.encode(query)[np.newaxis, :]\n",
    "    score,ind= index.search(query_emb,n)\n",
    "    return score,ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "generator_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_gen(query:str,cntxt_itm:list[dict])->str:\n",
    "    context=\"\\n\".join([itm[\"sentence_chunk\"]for itm in cntxt_itm])\n",
    "    base_prompt={\n",
    "        'question':query,\n",
    "        'context':context\n",
    "    }\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query):\n",
    "    _,indice=retrieve(query,embed_model,4)\n",
    "    cntxt_it=[pg_chunk[i] for i in indice.flatten()]\n",
    "    \n",
    "    prompt=prompt_gen(query,cntxt_it)\n",
    "    res=nlp(prompt)\n",
    "    print(res['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
